---
title: Emotion Recognition from Speech
publishDate: 2024-01-01
img: /assets/emotion.PNG
img_alt: Overview of the emotion recognition application
description: |
  Developed an application for recognizing emotions from audio recordings using a Long Short-Term Memory (LSTM) network. The project leverages machine learning to classify emotions expressed in speech, providing a user-friendly interface for interaction and visualization of results.

tags:
  - Machine Learning
  - Deep Learning
  - Flask
  - Python
  - Data Visualization
---

## Project Overview

### Context and Objectives

Emotion recognition from audio is a research area that aims to understand individuals' emotional states by analyzing their voice. This project focused on developing an application that utilizes a Long Short-Term Memory (LSTM) network to detect and classify emotions in audio recordings. The primary goals were to capture temporal dependencies in audio data and provide an intuitive user interface for model interaction.

### Data Preparation

The project utilized the TESS (Toronto emotional speech set) dataset from Kaggle, containing emotional speech recordings by Canadian English speakers. The emotions covered include joy, sadness, anger, fear, disgust, surprise, and neutrality. Prior to model training, audio files were preprocessed to extract essential features using Mel-Frequency Cepstral Coefficients (MFCCs), which transform audio signals into actionable numerical representations.

### Machine Learning Model

The model employed LSTM networks to handle the sequential nature of audio data. LSTMs are designed to manage temporal sequences and retain information over extended periods, crucial for processing audio signals. The model architecture includes multiple LSTM layers to capture information at various levels of granularity, followed by dense layers for final emotion classification.

### Model Training and Evaluation

The model was trained over 50 epochs with a batch size of 64, using training and validation sets to assess performance. Techniques such as checkpointing were implemented to prevent overfitting and retain the model with the highest accuracy on the validation set. The model achieved a remarkable accuracy of 99.29%, demonstrating its efficacy in predicting emotions from audio recordings.

### Application Development

The application was developed using Flask for the backend and web technologies like HTML, CSS, Bootstrap, and JavaScript for the frontend. Users can upload audio files, which are processed by the LSTM model to predict the emotions expressed. The results are displayed in a user-friendly interface, facilitating easy interaction with the model.

### Impact and Utility

This project significantly advances the field of vocal emotion recognition, with potential applications in sentiment analysis, human-computer interaction, and virtual assistance. Future improvements may include exploring other architectures like Convolutional Neural Networks (CNNs) and incorporating multilingual data for better generalization.

### Conclusion

The project demonstrates the potential of machine learning models, particularly LSTMs, for detecting emotions from audio. It provides a robust foundation for future developments and applications across various domains, despite the inherent challenges associated with the complexity and variability of human emotions.

-

## Screenshots

### Application Interface

<div id="carousel" style="display: flex; align-items: center; flex-direction: column; gap: 1rem;">
  <div style="position: relative; width: 80%;">
    <img id="carousel-img" src="/assets/emo1.PNG" alt="Emotion Recognition Interface" style="width: 100%; cursor: pointer;" onclick="openModal(0)" />
    <p><strong>Emotion Recognition Interface</strong>: Screenshot of the application interface where users upload audio files and view emotion predictions.</p>
  </div>
  <div style="position: relative; width: 80%;">
    <img id="carousel-img" src="/assets/emo2.PNG" alt="Emotion Prediction Results" style="width: 100%; cursor: pointer;" onclick="openModal(1)" />
    <p><strong>Emotion Prediction Results</strong>: Screenshot showing the results of emotion predictions from the audio recordings.</p>
  </div>
</div>
*
<div id="modal" style="display: none; position: fixed; top: 0; left: 0; width: 100%; height: 100%; background-color: rgba(0,0,0,0.8); justify-content: center; align-items: center;">
  <span style="position: absolute; top: 10px; right: 20px; font-size: 2rem; color: white; cursor: pointer;" onclick="closeModal()">&times;</span>
  <button style="position: absolute; left: 10px; top: 50%; font-size: 2rem; color: white; background: none; border: none; cursor: pointer;" onclick="prevImage()">&lt;</button>
  <img id="modal-img" src="" alt="Expanded View" style="max-width: 90%; max-height: 90%;" />
  <button style="position: absolute; right: 10px; top: 50%; font-size: 2rem; color: white; background: none; border: none; cursor: pointer;" onclick="nextImage()">&gt;</button>
</div>

<script>
  const images = [
    '/assets/emotion-recognition-screenshot1.png',
    '/assets/emotion-recognition-screenshot2.png'
  ];

  let currentIndex = 0;

  function openModal(index) {
    currentIndex = index;
    document.getElementById('modal-img').src = images[currentIndex];
    document.getElementById('modal').style.display = 'flex';
  }

  function closeModal() {
    document.getElementById('modal').style.display = 'none';
  }

  function prevImage() {
    currentIndex = (currentIndex > 0) ? currentIndex - 1 : images.length - 1;
    document.getElementById('modal-img').src = images[currentIndex];
  }

  function nextImage() {
    currentIndex = (currentIndex < images.length - 1) ? currentIndex + 1 : 0;
    document.getElementById('modal-img').src = images[currentIndex];
  }
</script>
